% vim:ts=1:et:nospell:spelllang=en_gb:ft=tex

 \chapter{Approach}

  In this chapter we explain the different approaches we tried in order to
  reach our goal and find a solution for the problem we described. As you will
  see, this was not immediately a straightforward process but rather one of
  trial and error. The goal was clear, the starting point was clear as well,
  but as often in computer science, there is more than one way to get from
  point A to point B, and it is not always clear which way is the best,
  easiest, most efficient or most effective.
 
  Since we are talking about the approach here, and not the implementation (for
  that, see chapter \ref{implementation}), we start by describing in broad
  terms what needs to be done and how this should be done, then we refine until
  we have a full set of specifications ready for implementation, where the last
  details will be ironed out.

  Unfortunately it is possible to refine an approach until it is ready for
  implementation, and only find out during implementation that the approach
  you have chosen will not work. This happened during our work on creating an
  automated layout system. Luckily we still had time to go back to the drawing
  board, and we did not have to restart from scratch; large parts of our
  approach were correct, the basic layout process we thought out was still a
  viable part of the approach, but it turned out we would have to split up the
  conversion and layout parts into two separate processes, rather than
  implementing them as two steps of the same process.
  
  Specifically, we had thought at first to figure out the ideal layout during
  conversion, when we would have all the separate components, by immediately
  putting them in the right place. This idea was partly conceived after looking
  at the HTML code generated by the \mxp compiler, thinking we would generate
  the same HTML code in our conversion process. It turned out we could bypass
  the \mxp compiler this way, but that would not be necessary: we could just as
  well generate \mxp XML and have the compiler take care of the rest for us.
 
  We also found during implementation that generating a layout in Java would
  not easily give us the results we were hoping for. However, at this point we
  had realized generating \mxp XML would be a better option, so we could have
  \mxp take care of the layout for us. Except \mxp did not do fully automated
  layout yet, the layout system was mostly template-based, so we decided we
  would need to write our own \mxp plug-in that would solve this problem for
  us.

  \section{Conversion process}

   The first part of the approach is fairly straightforward in its basic
   explanation: we had to convert \ppt presentations into \mxp presentations.
   This involved finding out how \ppt presentations are structured, getting the
   parts wee need out of that structure, and then putting those parts together
   in de \mxp structure.

   It appeared soon enough to us that the nature of this process resembled that
   of a compilation process. A compiler takes source code and transforms it
   into a working program with the semantics described by that source code. The
   compilation process consists of several steps. First the source code is
   tokenized, which means the symbols in the code are identified one by one and
   classified in certain categories.

   Then the tokens are processed by a parser into an intermediary form called a
   parse tree. A parser looks for certain predefined patterns in the source
   code. These patterns are part of the source code's language syntax. As such,
   these two steps analyse and validate the source code's syntax. If part of
   the code does not match any pattern, the parser and the compilation process
   stop and the user gets a message saying the code's syntax is invalid.

   When a parse tree is constructed, the compilation process can alter it, to
   improve it. Certain patterns in the parse tree may be replaceable by
   different patterns with the same outcome, but with more optimal execution.
   This part of the compilation process is optional, and is called compiler
   optimization. Optimizations can consist of many things, depending on the
   language. For example, some languages guarantee tail call optimization,
   where infinite loops can be constructed by letting a function call itself as
   its last statement without causing a stack overflow. This is something the
   compiler (or interpreter) can optimize during this part of the compilation
   process.

   After this, the parse tree can be written out to produce the desired output.
   Every node in the tree has a well-defined equivalent in the target
   language's syntax. The target language can be Assembly, which consists of
   the exact instructions a CPU needs to carry out a program, or it can be
   another programming language. Many compilers of higher-level languages
   translate their language into C, for several reasons: the C compilers that
   translate C into Assembly have been optimized so much that it is easier to
   rely on them than to put an enormous amount of effort into optimizing
   another language; C compilers exist for most --- if not all --- CPU
   architectures, which means translating a language into C makes it compatible
   with all those architectures, while it would cost a lot more effort to write
   different compilers for every architecture you would want to make your
   language available on.

   The conversion tool that is the purpose of this thesis, can be described in
   a similar succession of steps. As a first step, we take a \ppt presentation
   and take it apart into its components, effectively walking over each
   component, classifying them and registering their content type, original
   position and size, and any other specific properties. This can be seen as
   the tokenization phase, after which we end up with a series of `tokens' or,
   in our case, presentation components.
  
   We then turn this series of `tokens' into a `parse tree', an intermediary
   structure that reflects the relation between the components and the
   hierarchy of the presentation, which may consist of chapters, sections,
   slides and component groups. In \ppt this structure is fairly simple, so the
   creation of this `parse tree' is a straightforward process.
  
   However, in \mxp we are not limited to the rigid hierarchy of sections and
   slides, so at this point we can actually start manipulating our tree and
   improve upon it, for example by moving parts around, nesting components in
   different ways, grouping them in other ways than they originally were, and so on.
   In compilation terms, this is the optimization phase, where the compiler can
   manipulate the program to run more efficiently, to replace parts of it with
   other functionality, or to add features the source did not explicitly specify
   (e.g. garbage collection, but also spyware components \citep{scahill-1}). 

   As we discuss in section \ref{compiler-optimizations}, this seemed like the
   right time to incorporate automated layout generation into the conversion
   process. As we see later in section \ref{mxp-plug-in}, it turned out it
   was not. In the end, no significant `optimizations' or manipilation of the
   tree structure were implemented. Later on we would utilize this optimization
   phase to enable automated layout in another way, without actually performing
   the layout here, but at this point it would not affect the end result in any
   way.
  
   To finish the conversion process, we can traverse our component tree and
   generate a \mxp presentation from it. This can be done in several ways,
   since our intermediary form is in no way dependant on or bound to a specific
   format. Since the \mxp compiler was unavailable for a long time during our
   research and implementation, we decided it would be best to go straight to
   HTML5, so that we could test the conversion process without relying on the
   \mxp compiler. This worked out fairly well, although manually constructing
   HTML5 to work with the \mxp JavaScript library proved difficult. We ran into
   several issues, often mostly due to our lack of knowledge of the inner
   workings of \mxp, but we managed to get a presentable result that emulated
   the original \ppt presentation quite well.

   Afterwards, we altered our conversion tool to generate \mxp XML instead,
   which was a lot simpler since we would rely on \mxp to provide our layout
   and other things for us through the \mxp compiler. This approach allowed us
   to use the full power of \mxp, including our own plug-in for automated
   layout. At this point, the optimization phase was also revisited, and
   leveraged to introduce specific XML tags around component groups that would
   trigger our automated layout plug-in.

  \section{Compiler optimizations}
   \label{compiler-optimizations}

   Since the conversion process resembles that of a compiler, it seemed logical
   at first to make automated layout a part of that process, as some kind of
   `compiler optimization'. During this phase in the process, the component
   tree would be manipulated and altered, with the express purpose to improve
   upon its structure and properties, so as to get a better end result. Our
   improvements in this case would then consist of the automated layout.

   As a first attempt, we tried to traverse the component tree, giving each
   object new coordinates and sizes based on their original coordinates and
   sizes, as well as the coordinates and sizes of objects around them, so that
   they would fit together on every slide as well as possible. This seemed an
   easy solution, but the results were sub-optimal. On top of that, we soon
   realised that we were in essence creating another template-based system that
   would generate slides and presentations based on predefined ratios and
   rules, which was exactly the opposite of what we were trying to do. As such,
   we abandoned this approach in favor of a constraint-based algorithm as
   described in section \ref{related-algorithms}.

   This involved a technique that at first sight may seem like yet another
   template system, but actually is completely different: defining constraints
   for every component, in the form of margins, maximum sizes and other limits,
   and then calculating a way to satisfy all constraints while fitting content
   together on each slide. The similarities with template-based systems exist
   in the presence of predefined constraints, ratios and rules, but the
   important difference is that these constraints are defined relative to the
   component itself, without specifying anything absolute about location or
   size. For example, we would retain the aspect ratio of an image, without
   specifying its size, so that the image may be scaled to accomodate other
   components in a dynamic layout. As another example, we might specify there
   needs to be a certain distance between a component and any other components,
   relative to its size. We could also specify a certain relation between
   components, ensuring components stay in each others vicinity, one should
   always be left of the other, no other components may be placed between them,
   et cetera. Using these rules, we would then programmatically calculate the best
   layout using those components, but without any other bias. These constraints
   would be based only on the original situation, never on any suggestions from
   us or other developers or authors, which makes all the difference with
   traditional template-based layouts.
 
   While this is clearly a better method, it turned out the compiler
   optimization phase was not the best place in the process to take care of
   this. While we had the necessary data to calculate the layout, we would have
   had to generate the layout along with the \mxp presentation, after which the
   presentation could not be altered anymore without breaking the layout. This
   defeated the purpose of exporting to \mxp, which was to allow the presenter
   to edit, extend and improve their presentation further using \mxp. What we
   needed was a way to get \mxp itself to generate the layout, even if we
   wanted to add components to the presentation afterwards, and even if we
   wanted to create a new \mxp presentation instead of starting from \ppt.
   After all, how would we convince people to drop \ppt for \mxp's automated
   layout capabilities if they could only use that functionality by starting
   from \ppt?

   In the end, we decided to change our approach again. We took the automated
   layout out of the conversion process, instead opting at this point in the
   process to only add the necessary layout triggers in the form of an
   enclosing XML tag around the components that would need to be included in
   the automated layout. As such, the generated \mxp XML would include those
   tags, and a plug-in (described in section \ref{mxp-plug-in}) would then
   generate the layout at runtime.
  
  \section{Using \mxp}

   One of the primary goals of \mxp is to separate content from layout,
   allowing the author of a presentation to focus on the content while \mxp
   takes care of the layout. The way it does this is currently mostly through
   the compiler, which decides the width, height and coordinates of content,
   relative to the container the content belongs to. The plug-ins responsible
   for handling components and containers currently do not mess with those
   settings, but technically, they could. The compiler decides the measurements
   and coordinates based on templates. The solution we were looking for was a
   layout engine that could take any content and put it in an appropriate
   layout without any directions from the user. As such, we had to enhance
   \mxp's layout engine to use constraints, based on the size of the content,
   and try to find an optimal position for every component it is given.

   \subsection{A \mxp plug-in}
    \label{mxp-plug-in}

    We did this by creating an invisible container plug-in. Containers are a
    way of grouping components and other containers in \mxp. This means they
    have control over their child elements, which gives us the opportunity to
    override the layout of those elements. A container plug-in thus allows us
    to implement our own layout system. Since it is a new element, it does not
    override existing elements as it would have done if we had, for example,
    rewritten the `slide' plug-in. The user can decide for themself whether or
    not to use it, and it can be used anywhere in the presentation: wrap the
    whole presentation in it, or just a small part, whichever works best for
    your purposes. It also will not break existing presentations that do not
    use it, while those presentations can very easily be altered to take
    advantage of it.

    An important aspect of this is that containers can be nested. This means we
    can create slide-based presentations, which can contain our
    \code{autolayout} container, which then contains the slide's contents, thus
    creating an optimal layout of the content per-slide. Another way of using
    it could be without slides, throwing all content together in one
    \code{autolayout} container, and letting it take care of the layout for the
    whole presentation at once. It should be noted here that the
    \code{autolayout} container makes each of its child nodes focusable
    separately, to compensate for arbitrary resizing it may perform on large
    objects in order to fit them next to other content, by using the focus
    functionality to automatically zoom into these components when necessary.
  
    We call it an \emph{invisible} container plug-in because it does not
    introduce any visual content, shape or indication for itself. Compare with
    the \code{slide} plug-in which obviously puts some kind of slide-look
    around the content it encompasses, and it becomes clear what we mean by
    this: although the content within is obviously affected by our plug-in,
    there is no visible indication of its presence to the audience.

    The plug-in uses the compiler's numbers to decide relative locations
    between components, as well as size ratios, and then finds a way to display
    those components in a way that the display order makes sense (or at least
    matches the intended order as closely as possible), that no overlapping
    occurs (since we do not have the animations that \ppt might have used to
    display one piece of information and then another on top of it), and
    resizing everything if necessary in order to fit within the specified
    container. While this may seem like a bad idea since content can get
    illegibly small this way, keep in mind that we can rely on the
    ZUI\footnote{Zoomable User Interface} to focus on each component
    separately, or on groups of components, while \ppt obviously can only
    display the whole slide at once.

    In this manner we would generate \mxp presentations that were immediately
    usable, while also being adjustable; and on top of this, we would allow the
    automated layout process to be used in other \mxp presentations that were
    not originally converted from \ppt slides. The goal of this plug-in would
    thus be to provide automated layout functionality to \mxp presentations,
    and to allow any \mxp author to use it simply by putting an
    \code{<autolayout></autolayout>} container around the components they want
    the plug-in to act on. This approach has the additional advantage that the
    container can be used multiple times throughout the presentation, while
    also allowing other parts of the presentation to have a manual layout.
  
    Since it is possible to nest containers, which means a number of components
    could be grouped together in an \code{autolayout} container, then the
    result could be put into another \code{autolayout} container together with
    other components --- other \code{autolayout} containers, perhaps --- to
    generate an automated layout for an overview of the different groups.
    Compare it with traditional slideware, where components are grouped
    together in slides, then the slides might be put next to each other in an
    overview --- except our approach drops the slide boundaries, while still
    maintaining the ability to group components together to show a relation or
    link between them.

   \subsection{An automated layout algorithm}

    As discussed earlier, our first approach included an algorithm where
    content would be placed on slides according to certain rules, trying to
    attain a mythical `perfect' layout based on the golden ratio, symmetry,
    centering and other general guidelines we would find in advice on creating
    presentations. It turns out that, while following those guidelines as a
    human being is generally a good idea, a computer has different ways of
    calculating a good layout. The issue here can be compared to other problems
    in computer science; for example, people in the robotics department have
    tried for decades to create a robot that behaves exactly like a human
    being, and people in artificial intelligence have tried to create an AI
    that thinks like us. However, we have found time and time again that
    computers simply are not very good at acting, thinking or being human, just
    like we are not good at being computers. Making a computer act like a human
    makes it disadvantaged --- almost by definition, just like we are severely
    handicapped when we try to perform typically automatable, repetitive and/or
    math-intensive tasks. A computer's true power only shows when you let it do
    what it is good at, which is mostly the repetitive and the mathematically
    complex stuff. Trying to make it generate presentation layouts like a
    human would, is asking for subpar results.

    If we approach this problem keeping in mind a computer's strength and
    weaknesses, we arrive at a different approach. This involves calculating
    sizes, ratios, positions, margins and other numbers, of which the formulas
    are actually not too hard to come up with as a human, but which the
    execution is definitely more of a computer task. We start off by checking
    each component, and noting its original location and size. We then try to
    find components that are in proximity of each other, and figure out their
    original layout: above/below each other, next to each other, overlapping...
    Then, we try to put them together, possibly resizing them to match each
    other's sizes, and trying to match their original relative locations while
    introducing a certain rigidity, or consistency, by aligning them properly
    and puzzling them together as neatly as possible.
   
    This last part may sound weird, but it really is something to take into
    consideration, especially when the amount of components might be much
    bigger than what should fit on an average traditional slide. You could put
    all components in a row, just displaying them side-by-side, but that is not
    very aesthetically pleasing. Instead, we opted to try and keep components
    close to each other. This was finally achieved by finding the location
    closest to the starting point that would fit the component being
    considered, while still taking into account the earlier constraints about
    relative location and size. Thanks to the ZUI in \mxp, this makes for
    interesting layouts that still remain manageable, and provide a nice
    overview of all content when zoomed out.
