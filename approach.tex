% vim:ts=1:et:nospell:spelllang=en_gb:ft=tex

 \chapter{Approach}

  In this chapter we explain the different approaches we tried in order to
  reach our goal and find a solution for the problem we described. As you will
  see, this was not immediately a straightforward process but rather one of
  trial and error. The goal was clear, the starting point was clear as well,
  but as often in computer science, there is more than one way to get from
  point A to point B, and it is not always clear which way is the best,
  easiest, most efficient or most effective.
 
  Since we're talking about the approach here, and not the implementation (for
  that, see chapter \ref{implementation}), we start by describing in broad
  terms what needs to be done and how this should be done, then we refine until
  we have a full set of specifications ready for implementation, where the last
  details will be ironed out.

  Unfortunately it is possible to refine an approach until it is ready for
  implementation, and only find out during implementation that the approach
  you've chosen will not work. This happened during our work on creating an
  automated layout system. Luckily we still had time to go back to the drawing
  board, and we did not have to restart from scratch; large parts of our
  approach were correct, the basic layout process we thought out was still a
  viable part of the approach, but it turned out we would have to split up the
  conversion and layout parts into two separate processes, rather than
  implementing them as two steps of the same process.
  
  Specifically, we had thought at first to figure out the ideal layout during
  conversion, when we would have all the separate components, by immediately
  putting them in the right place. This idea was partly conceived after looking
  at the HTML code generated by the \mxp compiler, thinking we would generate
  the same HTML code in our conversion process. It turned out we could bypass
  the \mxp compiler this way, but that wouldn't be necessary: we could just as
  well generate \mxp XML and have the compiler take care of the rest for us.
 
  We also found during implementation that generating a layout in Java would
  not easily give us the results we were hoping for. However, at this point we
  had realized generating \mxp XML would be a better option, so we could have
  \mxp take care of the layout for us. Except \mxp didn't do fully automated
  layout yet, the layout system was mostly template-based, so we decided we
  would need to write our own \mxp plugin that would solve this problem for us.

  \section{Conversion process}

   The first part of the approach is fairly straightforward in its basic
   explanation: we had to convert \ppt presentations into \mxp presentations.
   This involved finding out how \ppt presentations are structured, getting the
   parts wee need out of that structure, and then putting those parts together
   in de \mxp structure.

   It appeared soon enough to us that the nature of this process resembled that
   of a compilation process. A compiler takes source code and transforms it
   into a working program with the semantics described by that source code. The
   compilation process consists of several steps. First the source code is
   tokenized, which means the symbols in the code are identified one by one and
   classified in certain categories.

   Then the tokens are processed by a parser into an intermediary form called a
   parse tree. A parser looks for certain predefined patterns in the source
   code. These patterns are part of the source code's language syntax. As such,
   these two steps analyse and validate the source code's syntax. If part of
   the code does not match any pattern, the parser and the compilation process
   stop and the user gets a message saying the code's syntax is invalid.

   When a parse tree is constructed, the compilation process can alter it, to
   improve it. Certain patterns in the parse tree may be replaceable by
   different patterns with the same outcome, but with more optimal execution.
   This part of the compilation process is optional, and is called compiler
   optimization. Optimizations can consist of many things, depending on the
   language. For example, some languages guarantee tail call optimization,
   where infinite loops can be constructed by letting a function call itself as
   its last statement without causing a stack overflow. This is something the
   compiler (or interpreter) can optimize during this part of the compilation
   process.

   After this, the parse tree can be written out to produce the desired output.
   Every node in the tree has a well-defined equivalent in the target
   language's syntax. The target language can be Assembly, which consists of
   the exact instructions a CPU needs to carry out a program, or it can be
   another programming language. Many compilers of higher-level languages
   translate their language into C, for several reasons: the C compilers that
   translate C into Assembly have been optimized so much that it is easier to
   rely on them than to put an enormous amount of effort into optimizing
   another language; C compilers exist for most --- if not all --- CPU
   architectures, which means translating a language into C makes it compatible
   with all those architectures, while it would cost a lot more effort to write
   different compilers for every architecture you would want to make your
   language available on.

   The conversion tool that is the purpose of this thesis, can be described in
   a similar succession of steps. As a first step, we take a \ppt presentation
   and take it apart into its components, effectively walking over each
   component, classifying them and registering their content type, original
   position and size, and any other specific properties. This can be seen as
   the tokenization phase, after which we end up with a series of `tokens' or,
   in our case, presentation components.
  
   We then turn this series of `tokens' into a `parse tree', an intermediary
   structure that reflects the relation between the components and the
   hierarchy of the presentation, which may consist of chapters, sections,
   slides and component groups. In \ppt this structure is fairly simple, so the
   creation of this `parse tree' is a straightforward process.
  
   However, in \mxp we are not limited to the rigid hierarchy of sections and
   slides, so at this point we can actually start manipulating our tree and
   improve upon it, for example by moving parts around, nesting components in
   different ways, grouping them in other ways than they originally were, etc.
   In compilation terms, this is the optimization phase, where the compiler can
   manipulate the program to run more efficiently, to replace parts of it with
   other functionality, or to add features the source didn't explicitly specify
   (e.g. garbage collection, but also spyware components \citep{scahill-1}). 

   As we discuss in section \ref{compiler-optimizations}, this seemed like the
   right time to incorporate automated layout generation into the conversion
   process. As we see later in section \ref{mxp-plugin}, it turned out it
   wasn't. In the end, no significant `optimizations' or manipilation of the
   tree structure were implemented. Perhaps in the future some other problem
   may be solved by utilizing this optimization phase, but in its current
   incarnation it does not affect the end result in any way.
  
   To finish the conversion process, we can traverse our component tree and
   generate a \mxp presentation from it. This can be done in several ways,
   since our intermediary form is in no way dependant on or bound to a specific
   format. Since the \mxp compiler was unavailable for a long time during our
   research and implementation, we decided it would be best to go straight to
   HTML5, so that we could test the conversion process without relying on the
   \mxp compiler. This worked out fairly well, although manually constructing
   HTML5 to work with the \mxp JavaScript library proved difficult. We ran into
   several issues, often mostly due to our lack of knowledge of the inner
   workings of \mxp, but we managed to get a presentable result that emulated
   the original \ppt presentation quite well.

   Afterwards, we altered our conversion tool to generate \mxp XML instead,
   which was a lot simpler since we would rely on \mxp to provide our layout
   and other things for us through the \mxp compiler. This approach allowed us
   to use the full power of \mxp, including our own plugin for automated
   layout. At this point, the optimization phase was also revisited, and
   leveraged to introduce specific XML tags around component groups that would
   trigger our automated layout plugin.

  \section{Compiler optimizations}
   \label{compiler-optimizations}

   Since the conversion process resembles that of a compiler, it seemed logical
   at first to make automatic layout a part of that process, as some kind of
   `compiler optimization'.

   At first, we tried to traverse the component tree, giving its objects new
   coordinates and sizes so that they would fit together on every slide as well
   as possible. This seemed an easy solution, but the results were sub-optimal.
   On top of that, we soon realised that we were in essence creating another
   template out of which a presentation would be made, which was exactly the
   opposite of what we were trying to do. As such, we abandoned this approach.

   We then switched to a different method: defining constraints for every
   component, in the form of margins, maximum sizes and other limits, and then
   calculating a way to satisfy all constraints while fitting content together
   on each slide. While this is clearly a better method, it turned out the
   compiler optimization phase was not the best place in the process to take
   care of this.

   In the end, we decided to take a different approach, relying on the layout
   engine of \mxp itself and enhancing that engine to create the automatic
   layout we wanteD.

  \section{Using \mxp}
   \label{mxp-plugin}

   One of the primary goals of \mxp is to separate content from layout,
   allowing the author of a presentation to focus on the content while \mxp
   takes care of the layout. The way it does this is currently mostly through
   the compiler, which decides the width, height and coordinates of content,
   relative to the container the content belongs to. The plug-ins responsible
   for handling components and containers currently don't mess with those
   settings, but technically, they could. The compiler decides the measurements
   and coordinates based on templates. The solution we were looking for was a
   layout engine that could take any content and put it in an appropriate
   layout without any directions from the user. As such, we had to enhance
   \mxp's layout engine to use constraints, based on the size of the content,
   and try to find an optimal position for every component it is given.

   We did this by creating an invisible container plug-in. Containers are a way
   of grouping components, other containers, etc. in \mxp. This means they have
   control over their child elements, which gives us the opportunity to
   override the layout of those elements. A container plugin thus allows us to
   implement our own layout system. Since it's a new element, it doesn't
   override existing elements as it would have done if we had, for example,
   rewritten the `slide' plugin. The user can decide for themself whether or
   not to use it, and it can be used anywhere in the presentation: wrap the
   whole presentation in it, or just a small part, whichever works best for
   your purposes. It also won't break existing presentations that don't use it,
   while those presentations can very easily be altered to take advantage of
   it.

   An important aspect of this is that containers can be nested. This means we
   can create slide-based presentations, which can contain our auto-layout
   container, which then contains the slide's contents, thus creating an
   optimal layout of the content per-slide. Another way of using it could be
   without slides, throwing all content together in one auto-layout container,
   and letting it take care of the layout for the whole presentation at once.
   It should be noted here that the auto-layout container makes each of its
   child nodes focusable separately, to compensate for arbitrary resizing it
   may perform on large objects in order to fit them next to other content, by
   using the focus functionality to automatically zoom into these components
   when necessary.
  
   We call it an \emph{invisible} container plug-in because it does not
   introduce any visual content, shape or indication for itself. Compare with
   the \emph{slide} plug-in which obviously puts some kind of slide-look around
   the content it encompasses, and it becomes clear what we mean by this:
   although the content within is obviously affected by our plug-in, there is
   no visible indication of its presence to the audience.

   The plug-in uses the compiler's numbers to decide relative locations between
   components, as well as size ratios, and then finds a way to display those
   components in a way that the display order makes sense (or at least matches
   the intended order as closely as possible), that no overlapping occurs
   (since we don't have the animations that \ppt might have used to display one
   piece of information and then another on top of it), and resizing everything
   if necessary in order to fit within the specified container. While this may
   seem like a bad idea since content can get illegibly small this way, keep in
   mind that we can rely on the ZUI to focus on each component separately, or
   on groups of components, while \ppt obviously can only display the whole
   slide at once.

%   TODO we need more content here

%   15:50 <omega> zeg, ik zit nu al een hele tijd thesis te schrijven en de laatste paar dagen vooral te zeveren over layout, maar intussen doe ik nog ni echt iets van layout, met t gedacht van ik schrijf daar binnenkort ne mindxpres plug-in voor en klaar
%   15:51 <omega> maar wordt layout momenteel eig ni mostly door de compiler gedaan?
%   15:52 <omega> ben zo eens naar de presentation.js libs en code gaan kijken, en ik zie ni direct een manier om ne plug-in layout te laten doen, aangezien plug-ins mostly component-specifiek zijn en ni alle componenten kunnen aansturen
%   15:53 <omega> dus klopt het dat ik ofwel de compiler moet aanpassen, ofwel presentation.js hacken om dat soort plug-ins toe te laten?
%   15:53 <omega> of laat het dat soort plug-ins al toe maar zijn er gewoon nog geen?
%   16:30 <omega> de 'structured' plug-in doet wel layout van slides, maar binnen die slides zie ik niet meteen een systeem dat layout regelt, met templates of otherwise, het pakt gewoon de coordinaten en afmetingen die de compiler bepaald heeft
%   16:31 <omega> al zou die slide plug-in wel *kunnen* prutsen met die layout... dus mss moet ik gwn de slide plug-in uitbreiden/hacken/vervangen
%   13:22 <reinout> ik zou een container plug-in maken
%   13:22 <reinout> gelijk de slide
%   13:22 <reinout> maar dan onzichtbaar
%   13:23 <reinout> want containers kan je nesten
%   13:23 <reinout> dus een slide kan bv uw layout container bevatten, die dan de children een layout geeft
%   13:23 <reinout> maar op die manier is uw layout ding bruikbaar buiten slides
%   13:24 <reinout> (alternatief was uw layout stuff in de slide plug-in steken)
%   13:31 <omega> oeh, cool idee indeed, beter dan de slide plug-in abusen
